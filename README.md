# üñºÔ∏èImageNet3D Flask app
# I. Project Background and Significance
In the field of computer vision, 3D understanding has always been a popular and challenging research direction. In recent years, with the rapid development of deep learning, synthetic images generated based on diffusion models come with pseudo 3D annotations. At first glance, it seems that a new "oil field" of data has been found for 3D research, which can help us more conveniently study the appearance of objects in three-dimensional space.<br>

The ImageNet3D Flask project emerges precisely based on the above-mentioned requirements. Its aim is to construct a high-quality 3D annotation dataset. This dataset is mainly used to evaluate the practical performance of 3D understanding models constructed based on large language models (LLMs) and generative diffusion models in scenarios with zero-shot or few-shot data, including core functions such as object localization, detection, and segmentation.<br>

For example, in the task of object localization, through the testing of this dataset, it is possible to examine the accuracy of the model in judging the position of objects in three-dimensional space when there is a lack of a large amount of annotated data. In the task of object detection, it is possible to assess the recognition efficiency and precision of the model for different objects in the scene.<br>

Many cutting-edge fields nowadays are inseparable from 3D vision technology. For instance, in autonomous driving, vehicles need to use 3D understanding technology to quickly identify pedestrians, vehicles, and obstacles on the road. <br>
In virtual reality, to provide users with an immersive experience, the 3D presentation of scenes and objects must be realistic enough. Similarly,<br>
in robot perception, accurate 3D data can help robots better understand their surrounding environment and complete complex operations.<br>

By accurately annotating attributes such as the position, rotation angle, and size of objects in 3D space, it is possible to provide high-quality data support for related research. <br>
<!--guowuquan-->
## üìñNotes on Using the Software and Solutions to Common Problems
### üì¶Packages to install before running the program:

- Flask <br>
- numpy <br>
- matplotlib <br>
- pillow <br>
- opencv-python-headless <br>
- flask-login <br>
- You can also install the required Python packages in one go using the following command:
pip install Flask numpy matplotlib pillow opencv-python-headless flask-login
<!-- by zhouxinain 2206302240333 ‰∏ÄÈîÆ‰∏ãËΩΩ-->
<br>

When in use, a folder named "imagene3dstorage" will be created in the current directory.  <br>
In the next-level directory of this folder, there will be an empty SQLite file with 0 bytes. <br>
You can replace its content with that of the "daatabasse_default.ssqlite" file in the software's root directory to use it normally. <br>
The software 'Navicat Premium' can be used for replacement. <br>
The file distribution is as follows:
## Repository Structure
'''
üìÅ ImageNet3D-Flask-app/
‚îú‚îÄ‚îÄ üìÅ static/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ images/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ tmp/
‚îú‚îÄ‚îÄ üìÅ templates/
‚îÇ   ‚îú‚îÄ‚îÄ login.html
‚îÇ   ‚îú‚îÄ‚îÄ account.html
‚îÇ   ‚îú‚îÄ‚îÄ test.html
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ eval.html
‚îÇ   ‚îú‚îÄ‚îÄ quality.html
‚îÇ   ‚îú‚îÄ‚îÄ doc.html
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ render.py
‚îú‚îÄ‚îÄ database.sqlite
‚îú‚îÄ‚îÄ database_default.sqlite
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ README.zh.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ üìÅ logs/
'''
<br>
<!-- by zhouxinian 2206302240333  The file distribution is as follows-->

## üììOverview
In the previous assignment we looked into synthetic images generated by diffusion models that come with pseudo 3D annotations<a href="#ref1">[1]</a>. We found that about 75% of the synthetic images are consistent with the pseudo 3D annotations while the other 25% have inconsistent viewpoint or shape. Although as a synthetic dataset it is a little noisy, we have shown that NMMs<a href="#ref2">[2]</a> can effectively learn from the "good" samples and boost performance on both in-distribution and out-of-distribution data.<br>

This opens many new opportunities for 3D understanding (e.g., grounding, detection, segmentation) -- models can be built on knowledge learned by existing LLMs and generative diffusion models. However, to analyze the performance of such models, we still need to develop a dataset with accurate 3D annotations to evaluate the zero-shot/few-shot performance of such models.<br>

**How do we describe an object in the 3D space?** In this project, we specify the 3D viewpoint of the object (see image below), the 2D location of the object (in the image plane), and the distance from the camera to the object. These six parameters allows us to fully specify the 3D location and 3D rotation of the object. Moreover, we are also interested in which subtype of the object is in the image -- if it is a car, is it a sedan or a hatchback. This is accomplished by associating the object with a best matching mesh model from a list of models given (you will see all the mesh models available in the bottom right corner of the annotation page).<br>

Lastly every real image comes with an existing annotation obtained from a pretrained 3D model. Your job would be refining the prediction of the model, as well as labeling the quality of the object. See below for a full list of "things to do" for each real image.<br>
<!-- ÊΩòÈíüË¥§ --> 

**‚ùóRead me ‚ò∫ ‚Üí Before you start, it is very important for you to go over this [tutorial](https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view) with qualitative examples demonstrating many important details. The first part of the tutorial is similar to this documentation and the second part of the tutorial provides a lot of qualitative images. Please go over a number of the examples in part two to make sure you fully grasp the idea.** <br>

![image](static/images/imagenet3d_viewpoint.png)

## ‚öôÔ∏èNew for v2 (Mar 2024)
1.As we start to deal with more challenging classes, we no longer have good initializations for the 3D orientation. New buttons are added with "‚Üì‚Üì" and "‚Üë‚Üë", which mean they are coarse buttons with bigger steps.<br>

2.Since we don't have good initializations, labeling one image may takes longer. Please be patient and make sure the results are visually correct.<br>

3.The quality of the images can be lower too -- some bounding boxes may not correspond to the class of interest. It is therefore important to label and disregard "bad" objects. Label the question as "bad quality/no object" so we will disregard this sample. In this case, you may skip tuning other parameters.<br>

4.Similar as before, it is important to label the visibility and the scene density. If you have gone through the [tutorial](https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view), you should have a good feeling of how to label these questions.<br>

### üé®Some other class-specific notes:<br>

- **bicycle-built-for-two:** We don't have good mesh models for tandem bicycles so we will be using mesh models of normal bicycles. This shouldn't be a problem as long as you match the 2D center and 3D orientation of the tandem bicycle in the image and the rendered mesh model.<br>

- **beaker:** There are different subtypes of beakers but we don't have good mesh models covering every shape. This wouldn't be a problem for the pose parameters we consider here. For 3D viewpoint, match the orientation of the beaker notch. 2D location and distance should be straightforward too.<br>

- **oboe:** It's almost impossible to tell the "azimuth" of an oboe. Just ignore this parameter and match the others.<br>

- **crutch:** There can be different types of crutches (see [here](https://www.physio-pedia.com/File:Types_of_crutches.jpg)). We only want the type that matches the mesh model we have (i.e., axillary crutch). For other types of crutches, just label them as "bad quality/no object".<br>
<!-- by pengyaoqing 2205308040322 -->

- **punching_bag:** There are different types of punching bags (see [here](https://img2.storyblok.com/1800x743/filters:focal(null):format(webp)/f/115220/2400x990/eae71daccc/how-to-choose-the-right-punching-bag-for-your-workout.jpeg)). We only want boxing bag and hanging bags, with a cylinder shape. For punching bags, also ignore the "azimuth" parameter.<br>
Please see the table below:
![ai_usage_screenshots](ai_usage_screenshots\2206302240333_2.png)
<!-- by zhouxinian 2206302240333  2206302240333_2-->

## ü§ñUesr Interface
Log in to the web app with your annotator ID. You will see the list of tasks assigned to you and track your progress in this page.<br>

![image](static/images/ui_login.png)

Clicking on a task will redirect you to the first unannotated question. On the top you will see basic information about this question, as well as multiple control buttons. Hovering on the buttons will show you what they do.<br>

- Move between questions with "<", ">", or ">>".<br>

- Delete current saved/unsaved annotation with the yellow reverse button.<br>

- Save the current annotaion with the green save button. (Or hit "Enter" as shortcut.)<br>

![image](static/images/ui_annotate.png)

The list of different CAD models are visualized on the bottom. Besides the 6D pose and type of model, you will also answer two questions on object quality and scene density.<br>

![image](static/images/ui_cads.png)

After labeling all information, clicking the green save button or pushing "Enter" key to save the annotation. If successful, you will see a message and the question status will be lighted.<br>

![image](static/images/ui_save.png)

# ImageNet3D Flask app
## Guidelines
***Matching mesh model (with initializations provided by a pretrained model).***
The best matching model from a list of mesh models given. This is crucial to make accurate estimations of the following parameters so this should be the first thing to do. Click on "<- Model" and "Model ->" buttons to change the mesh model selection.

***3D rotation (with initializations provided by a pretrained model).***
Three parameters are used to specify the 3D rotation of an object: azimuth, elevation, and in-plane rotation (theta). Adjust the three parameters so the 3D rotation of the rendered object aligns with the 3D rotation of the object in the image. Make sure you are not simply aligning the segmentation or boundaries of the two objects. You should focus on aligning the 3D rotation of the objects so the rendered object is "pointing" to the same direction as the object in the image.

***2D location (with initializations provided by a pretrained model).*** 2D location specifies the location of the center of the object.

***Distance (with initializations provided by a pretrained model).***
Distance between the object and the camera. To annotate the distance, make sure the "size" of the rendered object is roughly the same as the "size" of the object in the image.

***Object quality.***
 Object quality specifies how clearly the object is visible from the image. A "good" object would be clearly visible and not occluded. A "bad" object may be barely visible (imaging drving in heavy fog when cars in front of you are barely visible) or occluded by other objects. Several choices are considered:
 -  **Good.** Most part (more than 90%) of the object is clearly visible in the image.
 -  **Partially visible.** A small part of the object is occluded by other objects or outside the image (truncated by image boundary).
 -  **Barely visible.** only a small part of the object is clearly visible -- the other parts are either occluded or outside the image, or barely visible due to other reasons (e.g., weather).
 -  **Bad quality / no object.** Most part of the object is occluded or outside the image; or the pose of the object is very hard to tell.
 - Please see the table below:
    ![ai_usage_screenshots](ai_usage_screenshots\2206302240333_3.png)
    <!-- by zhouxinain 2206302240333 Ë°®2206302240333_3 -->

***Dense scene.*** 
This parameter tells if an object is very close to another object from the same category. Here "close" is defined in the 2D image plane -- two objects are close if the distance between them is small in the 2D image plane.

- **Not dense scene.** The object is not close to another object from the same category. There can be multiple objects from the same category in one image but the objects are far away from each other.
- **Dense scene.** The object is very close to another object from the same category. They may occlude each other or simply very close to each other.


<!-- ËíôÈúñÊòå --> 
<br>

## üå∞Examples
## Object Quality Evaluation Criteria
In this project, accurately evaluating object quality is of utmost importance for the annotation work. We categorize object quality into the following four types, each with a clear definition and examples to help you better understand and operate.

## Object Quality Categories
![image](static/images/78A2E58F9E46FD7A0E9CC1562D266577.png)
![image](static/images/961DD301331F918F562CE6C6B6103827.png)
- **Good**: In the image, most parts (more than 90%) of the object are clearly visible without significant occlusion. For example, the airplane in the top - left picture has its fuselage fully presented and is hardly occluded, falling into this category.<br>
- **Partially visible**: A small part of the object is occluded by other objects or extends beyond the image boundary. As shown in the top - right picture, the bottom of the cellphone is occluded by fingers, which meets the criteria for partially visible.<br>
- **Barely visible**: Only a tiny portion of the object is clearly distinguishable, while the remaining parts are occluded or outside the image. Just like the cellphone in the bottom - left picture, where most of it is not visible, it belongs to the barely visible category. <br>
- **Bad quality / no object**: Most parts of the object are occluded, outside the image, or due to reasons such as image blurriness or excessive darkness, it is difficult to determine the object's pose. For instance, in the bottom - right picture, since the ashtray cannot be seen, it should be labeled as this category.<br>

## Annotation Suggestions
When encountering objects that are barely visible or of bad quality / no object, although it is challenging to accurately determine their pose, you still need to make annotations based on your experience and imagination. Rest assured that the accuracy of the annotated pose does not have to be absolute. <br>
See [tutorial](https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view).<br>
<!--weirenwei -->
### ImageNet3D Project Annotation Guidelines - Taking Cellphones and Buckets as Examples<br>
In the ImageNet3D project, precise annotation is of utmost importance for data quality. The following are the key annotation guidelines for two object categories: cellphones and buckets.<br>

![image](static\images\20250513200816.png)
#### Cellphone<br>
- **CAD Model Coverage**: CAD models are expected to cover the majority of cellphone objects.<br>
- **Sample - Marking Criteria**:<br>
    - Mark the sample when the cellphone is barely visible.<br>
    - Mark the sample if only a small portion of the cellphone is visible.<br>
    - Mark the sample if the object is not a cellphone.<br>
![image](static\images\20250513200838.png)
#### Bucket<br>
- **CAD Model Coverage**: CAD models should cover most bucket objects.<br>
- **Model - Matching Adjustment**: In cases where the CAD model does not perfectly match (for instance, if the bucket in the image is slightly longer than the CAD model), make sure that the centers of the two buckets are aligned. Currently, many annotations align the top edge of the bucket rather than the center, and this needs to be rectified.<br>
- **3D Rotation Requirement**: Ensure that the 3D rotation is accurate. The projected CAD model on the image does not necessarily need to fully overlap with the bucket in the image.<br>

Adhering to these annotation rules can effectively enhance the accuracy and consistency of data annotation in the ImageNet3D project, thereby providing a reliable data basis for subsequent research and applications. <br>

## üìöReferences
<p id="ref1"></p>

[1] [Adding 3D Geometry Control to Diffusion Models](https://arxiv.org/abs/2306.08103) 
<br>

<p id="ref2"></p>

[2] [Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features](https://arxiv.org/abs/2209.05624) 
<br>
<!-- by zhuang xin jian 2205308040315 -->
